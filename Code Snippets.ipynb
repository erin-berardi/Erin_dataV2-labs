{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to combine values in rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combo(row):\n",
    "    if pd.isna(row['Type 2']):\n",
    "        return row['Type 1']\n",
    "    else:\n",
    "        return (row['Type 1'] + '-' + row['Type 2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to snake column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "for i in range(len(data.columns)):\n",
    "    cols.append(data.columns[i].lower().replace(' ', '_'))\n",
    "data.columns = cols\n",
    "\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop to plot multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in numerical.columns:\n",
    "    sns.distplot(numerical[column])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations and Heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = model_data.corr()\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=(10,8))\n",
    "ax=sns.heatmap(correlations, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test/split with shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "print('X_train shape is:',X_train.shape)\n",
    "print('y_train shape is:', y_train.shape)\n",
    "print('X_test shape is:', X_test.shape)\n",
    "print('y_train shape is:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encodes columns with lists as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Special features need to be lists to use\n",
    "\n",
    "X_train_cat = X_train_cat.drop('special_features',1).join(X_train_cat.special_features.str.join('|').str.get_dummies())\n",
    "X_train_cat.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical[\"coverage\"] = categorical[\"coverage\"].map({\"Basic\" : 0, \"Extended\" :.5, \"Premium\" : 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinMaxScaler code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = MinMaxScaler().fit(X_train_num)\n",
    "X_train_norm = transformer.transform(X_train_num)\n",
    "print(X_train_norm.shape)\n",
    "X_train_num_scale = pd.DataFrame(X_train_norm, index = X_train_num.index, columns=X_train_num.columns)\n",
    "X_train_num_scale.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE for imbalanced data code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_sampling(training_x, training_y):\n",
    "\n",
    "    smote = SMOTE(random_state = 100, k_neighbors = 3)\n",
    "    X_train_scaled_SMOTE, y_train_SMOTE = smote.fit_resample(training_x, training_y)\n",
    "\n",
    "    return X_train_scaled_SMOTE, y_train_SMOTE    \n",
    "\n",
    "X_train_SMOTE, y_train_SMOTE = over_sampling(X_train, y_train)\n",
    "X_test_SMOTE, y_test_SMOTE = over_sampling(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot-Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder to be used on new data later.\n",
    "\n",
    "encoder = OneHotEncoder(drop='first').fit(X_cat)\n",
    "\n",
    "cols = encoder.get_feature_names(input_features=X_cat.columns)\n",
    "\n",
    "X_cat_encode = pd.DataFrame(encoder.transform(X_cat).toarray(),columns=cols)\n",
    "\n",
    "X_cat_encode.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation codes R2, MSE, MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('train R2: {} -- test R2: {}'.format(linreg.score(X_train, y_train),\n",
    "                                            linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "\n",
    "train_mse=mse(linreg.predict(X_train), y_train)\n",
    "test_mse=mse(linreg.predict(X_test), y_test)\n",
    "\n",
    "print ('train MSE: {} -- test MSE: {}'.format(train_mse, test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('train RMSE: {} -- test RMSE: {}'.format(train_mse**.5, test_mse**.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "train_mae=mae(linreg.predict(X_train), y_train)\n",
    "test_mae=mae(linreg.predict(X_test), y_test)\n",
    "\n",
    "print ('train MAE: {} -- test MAE: {}'.format(train_mse, test_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(X, y):\n",
    "    #Finds and prints the metrics of the algorithm\n",
    "    predictions = lm.predict(X)\n",
    "    r2 = r2_score(y, predictions)\n",
    "    print('R2:', r2)\n",
    "    mse = np.sqrt(mean_squared_error(y,predictions))\n",
    "    print('MSE:', mse)\n",
    "    rmse = math.sqrt(mse)\n",
    "    print('RMSE:', rmse)\n",
    "    mae = mean_absolute_error(y, predictions)\n",
    "    print('MAE:', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to run multiple models to compare outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to run all models\n",
    "def models_automation(models, X_train, y_train):\n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        print(f\"{model.__class__.__name__}: Train -> {model.score(X_train, y_train)}, Test -> {model.score(X_test, y_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [LinearRegression(),SGDRegressor(),KNeighborsRegressor(), MLPRegressor(),DecisionTreeRegressor(),RandomForestRegressor()]\n",
    "models_automation(model_list, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for time/date transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_encoding(table):\n",
    "    #The input table needs to have a column 'effective_to_date'\n",
    "    table['month'] = table['effective_to_date'].dt.month\n",
    "    table['weekday'] = table['effective_to_date'].dt.day\n",
    "    table = table.drop('effective_to_date',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
